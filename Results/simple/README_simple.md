
target distribution: '110': 0.8, '001': 0.2
d=1:

initialized parameters: [2.56735406, 0.41999746, 4.78724587, 1.61450828, 1.94617788, 1.08221367,
 0.99579621, 4.56031104, 5.49933966]

Loss: Iteration 1: Loss = 1.2136500407689161
Iteration 2: Loss = 1.1105121691528186
Iteration 3: Loss = 0.9981565181771423
Iteration 4: Loss = 0.966659561692974
Iteration 5: Loss = 0.9117466252133424
Iteration 6: Loss = 0.8069691313902274
Iteration 7: Loss = 0.7262285181947161
Iteration 8: Loss = 0.6567723825631047
Iteration 9: Loss = 0.592577295030601
Iteration 10: Loss = 0.5115365106709074
Iteration 11: Loss = 0.511973638151505
Iteration 12: Loss = 0.4631953918995872
Iteration 13: Loss = 0.321457651608361
Iteration 14: Loss = 0.3313272886404761
Iteration 15: Loss = 0.31313017517425584
Iteration 16: Loss = 0.23326755900005078
Iteration 17: Loss = 0.25516787727438084
Iteration 18: Loss = 0.23700943855872605
Iteration 19: Loss = 0.2173990717803688
Iteration 20: Loss = 0.1832285697510163
Iteration 21: Loss = 0.19351061605402092
Iteration 22: Loss = 0.1904134445882932
Iteration 23: Loss = 0.14987312665815083
Iteration 24: Loss = 0.1209508432520148
Iteration 25: Loss = 0.10801956246503223
Iteration 26: Loss = 0.1455181893932096
Iteration 27: Loss = 0.11922697413117385
Iteration 28: Loss = 0.1187526963803265
Iteration 29: Loss = 0.11646230972551758
Iteration 30: Loss = 0.13455608692611468
Iteration 31: Loss = 0.1253980015861924
Iteration 32: Loss = 0.12545022069496545
Iteration 33: Loss = 0.13115182647000403
Iteration 34: Loss = 0.10800174948776764
Iteration 35: Loss = 0.09410616829553688
Iteration 36: Loss = 0.12716145129396506
Iteration 37: Loss = 0.07157704670428952
Iteration 38: Loss = 0.07309525594629948
Iteration 39: Loss = 0.08839149380672569
Iteration 40: Loss = 0.07396434891607608
Iteration 41: Loss = 0.07648838347093069
Iteration 42: Loss = 0.06517640393598229
Iteration 43: Loss = 0.07396434891607608
Iteration 44: Loss = 0.07648838347093069
Iteration 45: Loss = 0.06221893902992763
Iteration 46: Loss = 0.08950613811623609
Iteration 47: Loss = 0.07823653007333566
Iteration 48: Loss = 0.048719043693173744
Iteration 49: Loss = 0.07823653007333566
Iteration 50: Loss = 0.07144859247470405
Iteration 51: Loss = 0.051462454155564996
Iteration 52: Loss = 0.059373500488842865
Iteration 53: Loss = 0.08559424541177772
Iteration 54: Loss = 0.08559424541177772
Iteration 55: Loss = 0.0490197288366408
Iteration 56: Loss = 0.055304878798596135
Iteration 57: Loss = 0.07477491126075579
Iteration 58: Loss = 0.057991263689554554
Iteration 59: Loss = 0.07823653007333566
Iteration 60: Loss = 0.03615370395509779
Iteration 61: Loss = 0.051462454155564996
Iteration 62: Loss = 0.03810041648272451
Iteration 63: Loss = 0.03615370395509779
Iteration 64: Loss = 0.029896313764955537
Iteration 65: Loss = 0.024431371589204623
Iteration 66: Loss = 0.02096699348054923
Iteration 67: Loss = 0.04440300758688234
Iteration 68: Loss = 0.03159930585381754
Iteration 69: Loss = 0.03910181079256027
Iteration 70: Loss = 0.01967679308961255
Iteration 71: Loss = 0.02163217352922528
Iteration 72: Loss = 0.02163217352922528
Iteration 73: Loss = 0.019051396033929274
Iteration 74: Loss = 0.013478459524045673
Iteration 75: Loss = 0.03159930585381754
Iteration 76: Loss = 0.017839099198394867
Iteration 77: Loss = 0.01967679308961255
Iteration 78: Loss = 0.013478459524045673
Iteration 79: Loss = 0.018438890591405327
Iteration 80: Loss = 0.013478459524045673
Iteration 81: Loss = 0.00943389139615814
Iteration 82: Loss = 0.025166828176459433
Iteration 83: Loss = 0.006859658317276196
Iteration 84: Loss = 0.016114294210759904
Iteration 85: Loss = 0.005898687021544509
Iteration 86: Loss = 0.007546847276244745
Iteration 87: Loss = 0.006209837444694492
Iteration 88: Loss = 0.012033594513743924
Iteration 89: Loss = 0.002265976784261034
Iteration 90: Loss = 0.007546847276244745
Iteration 91: Loss = 0.006530127972803265
Iteration 92: Loss = 0.006859658317276196
Iteration 93: Loss = 0.008145836383334291
Iteration 94: Loss = 0.0032692765749759885
Iteration 95: Loss = 0.006209837444694492
Iteration 96: Loss = 0.002265976784261034
Iteration 97: Loss = 0.00046419074861698065
Iteration 98: Loss = 0.003052607401382027
Iteration 99: Loss = 0.0006352278992768235
Iteration 100: Loss = 5.050825467265308e-05

KL divergence for Gradient Descent: 5.050825467265308e-05

learning step=0.1

Generated Distribution gradient descent: (1000 times) Generated Distribution: {'001': 175, '101': 2, '010': 3, '110': 820}

KL divergence for COBYLA: 0.006149479556099582

Generated Distribution COBYLA: (1000 times) Generated Distribution: {'001': 168, '000': 1, '010': 5, '110': 826}

d=2:

initialized parameters: [3.3098439,  3.49193481, 1.10987926, 3.87140736, 5.06776035, 4.55036917
 ,1.61667775, 1.89392328, 3.7954585,  6.13347747, 4.19821141, 5.65592725,
 2.86883939, 1.81119494, 3.53953778, 5.60987419, 4.2260111,  4.17421696]

 Iteration 1, Loss: 0.6767867541058952
Iteration 2, Loss: 0.4958891148961696
Iteration 3, Loss: 0.425404757327681
Iteration 4, Loss: 0.3235572236375701
Iteration 5, Loss: 0.2613700178148857
Iteration 6, Loss: 0.22036352748468757
Iteration 7, Loss: 0.1845308137302799
Iteration 8, Loss: 0.14443184550867433
Iteration 9, Loss: 0.12567558968893858
Iteration 10, Loss: 0.09104085187093552
Iteration 11, Loss: 0.06945328080152335
Iteration 12, Loss: 0.07014727379190994
Iteration 13, Loss: 0.0606805458647984
Iteration 14, Loss: 0.054991941260060614
Iteration 15, Loss: 0.03735337514570863
Iteration 16, Loss: 0.03474654720770576
Iteration 17, Loss: 0.03134703193399153
Iteration 18, Loss: 0.02307460730606005
Iteration 19, Loss: 0.012516634408097393
Iteration 20, Loss: 0.008032680307333572
Iteration 21, Loss: 0.013218821159116715
Iteration 22, Loss: 0.010425885179238212
Iteration 23, Loss: 0.007937074736259772
Iteration 24, Loss: 0.011116717229785682
Iteration 25, Loss: 0.004131046505562737
Iteration 26, Loss: 0.004398298993823699
Iteration 27, Loss: 0.004018182108668506
Iteration 28, Loss: 0.0022729483742876654
Iteration 29, Loss: 0.0015289883586989354
Iteration 30, Loss: 0.00200313388603098
Iteration 31, Loss: 0.003041165943158352
Iteration 32, Loss: 0.004679436722544384
Iteration 33, Loss: 0.002364318785227344
Iteration 34, Loss: 0.0010323109600852273
Iteration 35, Loss: 0.0020199672882017365
Iteration 36, Loss: 0.0010252234031398978
Iteration 37, Loss: 0.00032064287866432825
Iteration 38, Loss: 0.0017133976262520903
Iteration 39, Loss: 0.00032064287866432825
Iteration 40, Loss: 3.132844363296387e-06
Iteration 41, Loss: 0.0
Iteration 42, Loss: 0.000834219950866525
Iteration 43, Loss: 7.912179634913131e-05
Iteration 44, Loss: 0.00015051948299700524
Iteration 45, Loss: 0.00046419074861698065
Iteration 46, Loss: 0.00032064287866432825
Iteration 47, Loss: 1.2438003861161423e-05
Iteration 48, Loss: 0.0033428965799359237
Iteration 49, Loss: 4.950799959942565e-05
Iteration 50, Loss: 0.00020413422151656715
Iteration 51, Loss: 0.0005462444347834692
Iteration 52, Loss: 0.0003890091396903366
Iteration 53, Loss: 0.0009700504498825653
Iteration 54, Loss: 3.132844363296387e-06
Iteration 55, Loss: 0.006223005878438853
Iteration 56, Loss: 0.00015588304701308602
Iteration 57, Loss: 0.0011861821083285623
Iteration 58, Loss: 0.0007699576267899359
Iteration 59, Loss: 0.00011085268832441465
Iteration 60, Loss: 3.1172191143310265e-06
Iteration 61, Loss: 0.0031557236370041465
Iteration 62, Loss: 0.0005922184962136555
Iteration 63, Loss: 1.2563011830577907e-05
Iteration 64, Loss: 0.002457755882437726
Iteration 65, Loss: 5.050825467265308e-05
Iteration 66, Loss: 0.0007311998355941391
Iteration 67, Loss: 0.0003890091396903366
Iteration 68, Loss: 0.0019196325087415776
Iteration 69, Loss: 0.000114229625980968
Iteration 70, Loss: 0.00032064287866432825
Iteration 71, Loss: 0.002450780525440472
Iteration 72, Loss: 0.00046419074861698065
Iteration 73, Loss: 0.0015662208578074883
Iteration 74, Loss: 0.0007699576267899359
Iteration 75, Loss: 0.00015051948299700524
Iteration 76, Loss: 1.2563011830577907e-05
Iteration 77, Loss: 0.00020413422151656715
Iteration 78, Loss: 0.0005118235681416394
Iteration 79, Loss: 5.050825467265308e-05
Iteration 80, Loss: 0.00046419074861698065
Iteration 81, Loss: 5.050825467265308e-05
Iteration 82, Loss: 0.0002590361878298548
Iteration 83, Loss: 0.000834219950866525
Iteration 84, Loss: 5.050825467265308e-05
Iteration 85, Loss: 0.0003890091396903366
Iteration 86, Loss: 0.0006352278992768235
Iteration 87, Loss: 0.0013114784437999634
Iteration 88, Loss: 0.0020889515915017177
Iteration 89, Loss: 0.000678275592489886
Iteration 90, Loss: 1.2563011830577907e-05
Iteration 91, Loss: 0.0011922104265670481
Iteration 92, Loss: 0.0004371286232433564
Iteration 93, Loss: 0.0019883567177104156
Iteration 94, Loss: 0.000304992931601758
Iteration 95, Loss: 0.00011085268832441465
Iteration 96, Loss: 0.00032064287866432825
Iteration 97, Loss: 1.2438003861161423e-05
Iteration 98, Loss: 0.0002590361878298548
Iteration 99, Loss: 1.2563011830577907e-05
Iteration 100, Loss: 0.0003681720720365711

KL divergence for Gradient Descent: 0.0003681720720365711

learning step=0.1

Generated Distribution gradient descent: (1000 times) Generated Distribution: Generated Distribution: {'001': 179, '110': 821}

KL divergence for COBYLA: 0.0009700504498825653

Generated Distribution COBYLA: (1000 times) Generated Distribution: Generated Distribution: {'001': 189, '101': 1, '110': 810}
