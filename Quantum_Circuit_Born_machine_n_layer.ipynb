{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cddd2fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import Aer, QuantumCircuit, transpile, execute\n",
    "from qiskit.circuit import Parameter\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from qiskit.visualization import plot_histogram\n",
    "from Target_Distibution import TARGET_FUNCTION\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f9ae42",
   "metadata": {},
   "source": [
    "### Define the QCBM Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ccf3d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qcbm_circuit(params, num_qubits, num_layers=2):\n",
    "    \"\"\"\n",
    "    Construct a QCBM circuit with the specified number of layers (d).\n",
    "    \n",
    "    Args:\n",
    "        params (np.ndarray): Array of parameters (size: num_qubits * 3 * num_layers).\n",
    "        num_qubits (int): Number of qubits.\n",
    "        num_layers (int): Number of layers (d).\n",
    "        \n",
    "    Returns:\n",
    "        QuantumCircuit: The parameterized QCBM circuit.\n",
    "    \"\"\"\n",
    "    qc = QuantumCircuit(num_qubits)\n",
    "    idx = 0\n",
    "\n",
    "    for layer in range(num_layers):\n",
    "        # Single-qubit rotations for each qubit in the layer\n",
    "        for q in range(num_qubits):\n",
    "            qc.rz(params[idx], q)\n",
    "            qc.rx(params[idx + 1], q)\n",
    "            qc.rz(params[idx + 2], q)\n",
    "            idx += 3\n",
    "        # Add entangling gates\n",
    "        for q in range(num_qubits - 1):\n",
    "            qc.cx(q, q + 1)\n",
    "    return qc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020f8436",
   "metadata": {},
   "source": [
    "### Define the Loss Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "025d469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(params, num_qubits, num_layers, target_distribution):\n",
    "    \"\"\"\n",
    "    Calculate the KL divergence between target and generated distributions.\n",
    "    \n",
    "    Args:\n",
    "        params (np.ndarray): Current parameters of the QCBM circuit.\n",
    "        num_qubits (int): Number of qubits.\n",
    "        num_layers (int): Number of layers (d).\n",
    "        target_distribution (dict): Target probability distribution.\n",
    "        \n",
    "    Returns:\n",
    "        float: KL divergence.\n",
    "    \"\"\"\n",
    "    qc = qcbm_circuit(params, num_qubits, num_layers)\n",
    "    qc.measure_all()\n",
    "\n",
    "    # Simulate the circuit\n",
    "    simulator = Aer.get_backend('qasm_simulator')\n",
    "    qc = transpile(qc, simulator)\n",
    "    result = execute(qc, backend=simulator, shots=8000).result()\n",
    "    counts = result.get_counts()\n",
    "    \n",
    "    # Convert counts to probabilities\n",
    "    generated_distribution = {k: v / 8000 for k, v in counts.items()}\n",
    "    \n",
    "    # Calculate KL divergence\n",
    "    kl_divergence = 0\n",
    "    for bitstring, target_prob in target_distribution.items():\n",
    "        generated_prob = generated_distribution.get(bitstring, 1e-10)\n",
    "        kl_divergence += target_prob * np.log(target_prob / generated_prob)\n",
    "    \n",
    "    return kl_divergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e5ec0b",
   "metadata": {},
   "source": [
    "### Define/Compute Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9376936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_computer(params,params_shift,num_qubits,  num_layers, target_distribution):\n",
    "    qc_a = qcbm_circuit(params, num_qubits, num_layers)\n",
    "    qc_a.measure_all()\n",
    "\n",
    "    simulator = Aer.get_backend('qasm_simulator')\n",
    "    qc_a = transpile(qc_a, simulator)\n",
    "    result_a = execute(qc_a, backend=simulator, shots=8000).result()\n",
    "    counts_a = result_a.get_counts()\n",
    "    \n",
    "    # Normalize counts to probabilities\n",
    "    generated_distribution_a = {k: v / 8000 for k, v in counts_a.items()}\n",
    "    \n",
    "    qc_b = qcbm_circuit(params_shift, num_qubits, num_layers)\n",
    "    qc_b.measure_all()\n",
    "\n",
    "    simulator = Aer.get_backend('qasm_simulator')\n",
    "    qc_b = transpile(qc_b, simulator)\n",
    "    result_b = execute(qc_b, backend=simulator, shots=8000).result()\n",
    "    counts_b = result_b.get_counts()\n",
    "\n",
    "    # Normalize counts to probabilities\n",
    "    generated_distribution_b = {k: v / 8000 for k, v in counts_b.items()}\n",
    "    \n",
    "    gradient_component=0\n",
    "    for bitstring, target_prob in target_distribution.items():\n",
    "        generated_prob_a = generated_distribution_a.get(bitstring, 1e-10)\n",
    "        generated_prob_b = generated_distribution_b.get(bitstring, 1e-10)\n",
    "        gradient_component -=  target_prob /generated_prob_a*generated_prob_b\n",
    "        \n",
    "    return gradient_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f7685c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(params, num_qubits, num_layers, target_distribution):\n",
    "    \"\"\"\n",
    "    Compute gradients of the loss function using the parameter-shift rule.\n",
    "    \n",
    "    Args:\n",
    "        params (np.ndarray): Current parameters of the QCBM circuit.\n",
    "        num_qubits (int): Number of qubits.\n",
    "        num_layers (int): Number of layers (d).\n",
    "        target_distribution (dict): Target probability distribution.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Gradients of the loss function w.r.t. parameters.\n",
    "    \"\"\"\n",
    "    gradients = np.zeros_like(params)\n",
    "    shift = np.pi / 2  # Parameter shift value\n",
    "\n",
    "    for i in range(len(params)):\n",
    "        # Shift parameter up\n",
    "        params_shifted_up = np.copy(params)\n",
    "        params_shifted_up[i] += shift\n",
    "        loss_up = gradient_computer(params, params_shifted_up, num_qubits, num_layers, target_distribution)\n",
    "\n",
    "        # Shift parameter down\n",
    "        params_shifted_down = np.copy(params)\n",
    "        params_shifted_down[i] -= shift\n",
    "        loss_down = gradient_computer(params, params_shifted_down, num_qubits, num_layers, target_distribution)\n",
    "\n",
    "        # Compute gradient\n",
    "        gradients[i] =loss_up - loss_down\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b54f159",
   "metadata": {},
   "source": [
    "### Gradient Descent Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59b5dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(initial_params, num_qubits, num_layers, target_distribution, learning_rate=0.1, max_iters=100):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to optimize QCBM parameters.\n",
    "    \n",
    "    Args:\n",
    "        initial_params (np.ndarray): Initial QCBM parameters.\n",
    "        num_qubits (int): Number of qubits.\n",
    "        num_layers (int): Number of layers (d).\n",
    "        target_distribution (dict): Target probability distribution.\n",
    "        learning_rate (float): Learning rate for parameter updates.\n",
    "        max_iters (int): Maximum number of iterations.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Optimized parameters.\n",
    "    \"\"\"\n",
    "    params = np.copy(initial_params)\n",
    "    for iteration in range(max_iters):\n",
    "        # Compute gradients\n",
    "        gradients = compute_gradients(params, num_qubits, num_layers, target_distribution)\n",
    "        \n",
    "        # Update parameters\n",
    "        params -= learning_rate * gradients\n",
    "        \n",
    "        # Compute loss for monitoring\n",
    "        loss = loss_function(params, num_qubits, num_layers, target_distribution)\n",
    "        print(f\"Iteration {iteration + 1}, Loss: {loss}\")\n",
    "        if loss<0.1:\n",
    "            break\n",
    "    \n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f6fc4a",
   "metadata": {},
   "source": [
    "### Training the QCBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb812a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Train the QCBM\n",
    "num_qubits = 6\n",
    "num_layers = 2\n",
    "num_params = num_qubits * 3 * num_layers\n",
    "initial_params = np.random.uniform(0, 2 * np.pi, num_params)\n",
    "\n",
    "# Define target distribution\n",
    "target_distribution = TARGET_FUNCTION.gaussian(num_qubits)\n",
    "\n",
    "# Train using gradient descent\n",
    "optimized_params = gradient_descent(\n",
    "    initial_params, \n",
    "    num_qubits, \n",
    "    num_layers, \n",
    "    target_distribution, \n",
    "    learning_rate=0.1, \n",
    "    max_iters=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41496e03",
   "metadata": {},
   "source": [
    "### Constuct Citcuit Using Trained Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e2c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_qcbm = qcbm_circuit(optimized_params, num_qubits, num_layers)\n",
    "trained_qcbm.measure_all()\n",
    "\n",
    "simulator = Aer.get_backend('qasm_simulator')\n",
    "trained_qcbm = transpile(trained_qcbm, simulator)\n",
    "result = execute(trained_qcbm, backend=simulator, shots=8000).result()\n",
    "counts = result.get_counts()\n",
    "\n",
    "print(\"Generated Distribution:\", counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4997a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_qcbm.draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c861edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ddcdd5",
   "metadata": {},
   "source": [
    "### Compare Target Distribution & Generated Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b635aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and sort keys\n",
    "all_keys = sorted(set(target_distribution.keys()).union(counts.keys()))\n",
    "\n",
    "# Extract values for both distributions, setting missing ones to 0\n",
    "generated_values = np.array([counts.get(key, 0) for key in all_keys])\n",
    "target_values = np.array([target_distribution.get(key, 0) for key in all_keys])\n",
    "generated_values = generated_values/8000\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot generated distribution as a histogram\n",
    "plt.bar(all_keys, generated_values, color='blue', alpha=0.6, label='Generated Distribution')\n",
    "\n",
    "# Plot target distribution as a line plot\n",
    "plt.plot(all_keys, target_values, marker='o', linestyle='-', color='orange', label='Target Distribution')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Binary Strings')\n",
    "plt.ylabel('Frequencies')\n",
    "plt.title('Generated Distribution (Histogram) vs Target Distribution (Line Plot)')\n",
    "plt.legend()\n",
    "\n",
    "# Add grid and adjust layout\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b5de17",
   "metadata": {},
   "source": [
    "### Another Method: Calculating Gradient Using Existing Function, minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4791423",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qubits = 6\n",
    "num_layers = 2\n",
    "num_params = num_qubits * 3 * num_layers\n",
    "initial_params = np.random.uniform(0, 2 * np.pi, num_params)\n",
    "\n",
    "# Define target distribution (e.g., |110⟩ with 0.8 probability, |001⟩ with 0.2 probability)\n",
    "target_distribution = TARGET_FUNCTION.gaussian(num_qubits)\n",
    "\n",
    "result_cob = minimize(loss_function, initial_params, args=(num_qubits,num_layers, target_distribution), method='COBYLA')\n",
    "\n",
    "# Optimized parameters\n",
    "optimized_params_cob = result_cob.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399e35a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_qcbm_cob = qcbm_circuit(optimized_params_cob, num_qubits, num_layers)\n",
    "trained_qcbm_cob.measure_all()\n",
    "trained_qcbm_cob.draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd16a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = Aer.get_backend('qasm_simulator')\n",
    "trained_qcbm = transpile(trained_qcbm_cob, simulator)\n",
    "result = execute(trained_qcbm_cob, backend=simulator, shots=8000).result()\n",
    "counts = result.get_counts()\n",
    "\n",
    "print(\"Generated Distribution:\", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de061559",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f0e1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(target_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3dcdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and sort keys\n",
    "all_keys = sorted(set(target_distribution.keys()).union(counts.keys()))\n",
    "\n",
    "# Extract values for both distributions, setting missing ones to 0\n",
    "generated_values = np.array([counts.get(key, 0) for key in all_keys])\n",
    "target_values = np.array([target_distribution.get(key, 0) for key in all_keys])\n",
    "generated_values = generated_values/8000\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot generated distribution as a histogram\n",
    "plt.bar(all_keys, generated_values, color='blue', alpha=0.6, label='Generated Distribution')\n",
    "\n",
    "# Plot target distribution as a line plot\n",
    "plt.plot(all_keys, target_values, marker='o', linestyle='-', color='orange', label='Target Distribution')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Binary Strings')\n",
    "plt.ylabel('Frequencies')\n",
    "plt.title('Generated Distribution (Histogram) vs Target Distribution (Line Plot)')\n",
    "plt.legend()\n",
    "\n",
    "# Add grid and adjust layout\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c97bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qiskit0.46",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
